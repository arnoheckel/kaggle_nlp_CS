{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politics\n",
      "Health\n",
      "Finance\n",
      "Travel\n",
      "Food\n",
      "Education\n",
      "Environment\n",
      "Fashion\n",
      "Science\n",
      "Sports\n",
      "Technology\n",
      "Entertainment\n"
     ]
    }
   ],
   "source": [
    "#printing data from json file\n",
    "\n",
    "\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "file_path = \"data/train.json\"\n",
    "\n",
    "# Open the JSON file\n",
    "with open(file_path) as file:\n",
    "    # Load the JSON data\n",
    "    data = json.load(file)\n",
    "\n",
    "for element in data:\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three examples per category which is not a lot. The examples are short sentences which makes it almost impossible for td-idf and word2vec to work properly. Training seems difficult so we could try few shot learning for example, we could use a model as encoder and then usethe right distance to compare to the few examples we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The mayor announced a new initiative to improve public transportation.', 'The senator is facing criticism for her stance on the recent bill.', 'The upcoming election has sparked intense debates among the candidates.', 'Regular exercise and a balanced diet are key to maintaining good health.', 'The World Health Organization has issued new guidelines on COVID-19.', 'A new study reveals the benefits of meditation for mental health.', 'The stock market saw a significant drop following the announcement.', 'Investing in real estate can be a profitable venture if done correctly.', \"The company's profits have doubled since the launch of their new product.\", 'Visiting the Grand Canyon is a breathtaking experience.', 'The tourism industry has been severely impacted by the pandemic.', 'Backpacking through Europe is a popular choice for young travelers.', 'The new restaurant in town offers a fusion of Italian and Japanese cuisine.', 'Drinking eight glasses of water a day is essential for staying hydrated.', 'Cooking classes are a fun way to learn new recipes and techniques.', 'The school district is implementing a new curriculum for the upcoming year.', 'Online learning has become increasingly popular during the pandemic.', 'The university is offering scholarships for students in financial need.', 'Climate change is causing a significant rise in sea levels.', 'Recycling and composting are effective ways to reduce waste.', 'The Amazon rainforest is home to millions of unique species.', 'The new fashion trend is all about sustainability and eco-friendly materials.', 'The annual Met Gala is a major event in the fashion world.', 'Vintage clothing has made a comeback in recent years.', \"NASA's Mars Rover has made significant discoveries about the red planet.\", 'The Nobel Prize in Physics was awarded for breakthroughs in black hole research.', 'Genetic engineering is opening up new possibilities in medical treatment.', 'The NBA Finals are set to begin next week with the top two teams in the league.', 'Serena Williams continues to dominate the tennis world with her powerful serve.', 'The World Cup is the most prestigious tournament in international soccer.', 'Artificial intelligence is changing the way we live and work.', 'The latest iPhone has a number of exciting new features.', 'Cybersecurity is becoming increasingly important as more and more data moves online.', 'The new Marvel movie is breaking box office records.', 'The Grammy Awards are a celebration of the best music of the year.', 'The latest season of Game of Thrones had fans on the edge of their seats.']\n"
     ]
    }
   ],
   "source": [
    "#Premiers tests avec TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a list of sentences from json file \n",
    "\n",
    "sentences = []\n",
    "\n",
    "for element in data:\n",
    "    for i in data[element]:\n",
    "        sentences.append(i)\n",
    "\n",
    "#print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.02763934 0.03092786 0.03299303\n",
      " 0.03318711 0.04813076 0.06174145 0.06223583 0.06223583 0.07598174\n",
      " 0.08656039 0.09263897 0.09690515 0.09890436 0.1102567  0.11777686\n",
      " 0.12890202 0.13902519 0.14353748 0.14738137 0.15277842 0.15362058\n",
      " 0.15486086 0.17471614 0.18756061 0.20357324 0.20409224 0.21248346\n",
      " 0.21257964 0.21529027 0.25866692 0.28205742 0.28772251 0.34645899]\n",
      "[0.21529027 0.25866692 0.28205742 0.28772251 0.34645899]\n",
      "The annual Met Gala is a major event in the fashion world.\n",
      "The stock market saw a significant drop following the announcement.\n",
      "The Grammy Awards are a celebration of the best music of the year.\n",
      "The latest season of Game of Thrones had fans on the edge of their seats.\n",
      "Climate change is causing a significant rise in sea levels.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"The role of credit scores in lending decisions is significant.\"\n",
    "query_vec = vectorizer.transform([query])\n",
    "results = cosine_similarity(X,query_vec)\n",
    "\n",
    "\n",
    "\n",
    "highest_indices = np.argsort(results.flatten())[-5:]\n",
    "highest_values = results.flatten()[highest_indices]\n",
    "\n",
    "sorted_indices = np.argsort(results.flatten())\n",
    "sorted_results = results.flatten()[sorted_indices]\n",
    "\n",
    "print(sorted_results)\n",
    "print(highest_values)\n",
    "\n",
    "\n",
    "for i in highest_indices:\n",
    "    print(sentences[i])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification doesn't seem to work properly which seems logical because there are no common words between query and the sentences on which the model has been trained. The \"documents\" are too short and doesn't contain enough elements to make it work.  Let's try wordTovec on our training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "cpu = cpu_count()\n",
    "print('The virtual instance has {} cpus, that will be used to train the word2vec model'.format(cpu))\n",
    "\n",
    "# We will just get the \"WordVectors\" parameter from the trained Word2Vec model.\n",
    "# Otherwise, we could continue training with some more exemples that could be\n",
    "# fed on the fly to the model.\n",
    "print(\"Training the W2V ...\")\n",
    "pol = Word2Vec(, vector_size=100, window=5, min_count=3, workers=cpu)\n",
    "pol.train(cleaned_pol, total_examples=len(cleaned_pol), epochs=10)\n",
    "pol_wv = pol.wv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to use a pre-trained model from hugging face as a few shot classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's make a first try with distilbert \n",
    "\n",
    "from transformers import DistilBertForMaskedLM, DistilBertConfig, DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "config = DistilBertConfig(vocab_size_or_config_json_file = \"config.json\")\n",
    "\n",
    "model =  DistilBertModel(config)\n",
    "\n",
    "dictio = torch.load(\"pytorch_model.bin\", map_location=torch.device('cpu'))\n",
    "\n",
    "#We remove \"distilbert\" form the fields names because it is not in the model state_file \n",
    "dictio = {k.replace(\"distilbert.\",\"\"):v for k,v in dictio.items()}\n",
    "\n",
    "#We remove the pre-classifier and classifier weights\n",
    "dictio = {k:v for k,v in dictio.items() if \"pre_classifier\" not in k and \"classifier\" not in k} \n",
    "\n",
    "\n",
    "\n",
    "model.load_state_dict(dictio)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_old_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
